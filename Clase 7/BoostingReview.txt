Ambos son algoritmos de refuerzo, que convierten a un conjunto de modelos débiles en un único modelo mas fuerte. Ambos inicializan generalmente un árbol de decisiones y crean de manera iterativa un arbol débil que se agrega al arbol fuerte. 

Gradient Boosting es también un algoritmo de impulso, Por lo tanto, también intenta crear un aprendiz fuerte a partir de un conjunto de aprendices débiles. Este algoritmo es similar a Adaptive Boosting (AdaBoost) pero difiere de él en ciertos aspectos. En este método, intentamos visualizar el problema de impulso como un problema de optimización, es decir, asumimos una función de pérdida y tratamos de optimizarlo. Esta idea fue desarrollada por primera vez por Leo Breiman.

XGBoost tiene una estructura similar a Gradient Boosting Classifier, sin embargo cuenta con una serie de características particulares que potencian su rendimiento. Dentro de estas encontramos que los árboles de decisión que se crean con este algoritmo varían en cada iteración, tanto en la cantidad de nodos terminales como en el peso de cada una de las ramas. Adicional a esto, aplican extra randomisation parameter para reducir la correlación entre cada uno de los árboles generados, lo que nos permite tener una mejor desempeño para el modelo, ya que entre menos relación tengamos entre los árboles mejor será en ensamble de los mismos. Además, este algoritmo tiende a ser más rápido que Gradient Boosting Classifier.
